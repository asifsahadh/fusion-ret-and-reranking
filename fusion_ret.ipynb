{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acb9a8d",
   "metadata": {},
   "source": [
    "## **Fusion Retrieval based RAG** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ea273",
   "metadata": {},
   "source": [
    "_**What are your expectations for Fusion Retrieval with the provided manual?**_<br>\n",
    "Due to the hybrid nature, semantic matching and keyword matching, I expect this architecture to be quite robust compared to vanilla RAG. Both types of matching has its benefits when it comes to information retrieval and I would argue that this architecture would outperform the Reranking RAG architecture. However, semantic search may be considered superior to keyword matching so it would be better to have a higher weightage given to semantics. This theory requires testing of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96226c83",
   "metadata": {},
   "source": [
    "**_How do you plan to test and compare these techniques?_**<br><br>\n",
    "<img src=\"./fusion_ret._workflow.png\" alt=\"Flowchart\" width=\"700\" /><br><br>\n",
    "The main approach taken here is where the initial chunks from the document are made into two copies, one is semantic based (using BERT), and the other is keyword based (BM25). After the top-K chunks are retrieved from both, a union operation is performed between the two. Next, this final set of chunks are again made into 2 copies of BERT based embeddings and BM25 based inverted indexes. The query is passed into both for scoring and the scores are retrieved for each chunk. This gives us a set of chunk IDs, BERT scores, BM25 scores and the final fusion score. This is the main idea and the process goes as follows:<br>\n",
    "1. The document data is extracted, specifically text and tabular data. \n",
    "2. Next, these data are stored in a way where the sequence is maintained, that way there will be more context for a certain text that may have a table before or after it. \n",
    "3. This set is chunked and converted to BERT based vector embeddings and also to BM25 based representations.\n",
    "4. Now the query is also converted to BERT based embeddings and BM25 based representations and the top-K chunks are retrieved for both. A union is taken between the retrieved chunks to avoid overlaps.\n",
    "5. Next, this final set of chunks are again made into 2 copies, one of BERT based embeddings and the other of BM25 based inverted indexes. The query is passed into both structures and the scores are retrieved for each chunk. This gives us a set of chunk IDs, BERT scores, BM25 scores and the final fusion score. \n",
    "6. Based on this fusion score, the top-L chunks are retrieved (L < K). \n",
    "7. These L chunks, which acts as context, along with the original query is passed into an LLM for a refined response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469864b",
   "metadata": {},
   "source": [
    "_**Comparison Strategy**_<br>\n",
    "There are possibly two main ways in which we can compare this approach with the Reranking apprach. One is by assessing the top-L retrieved chunks and the other is obviously by assessing the final response from the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756d5e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42e185",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # for text extraction\n",
    "import camelot # for table extraction\n",
    "from sentence_transformers import SentenceTransformer, util # for semantic vector embedding creation \n",
    "from rank_bm25 import BM25Okapi # for bm25 implementation\n",
    "import spacy # for stop word removal\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaeb13",
   "metadata": {},
   "source": [
    "#### 1. Function to extract texts & tables from PDF\n",
    "The goal is to preserve the sequence, that way there will be more context for a certain text that may have a table before or after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0193147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_tables(pdf_path):\n",
    "\n",
    "    pdf_file = Path(pdf_path)\n",
    "    if not pdf_file.is_file() or pdf_file.suffix.lower() != \".pdf\":\n",
    "        raise FileNotFoundError(\"Provided file path is not a valid PDF.\")\n",
    "\n",
    "    doc = fitz.open(str(pdf_file))\n",
    "    result = []\n",
    "\n",
    "    # text extraction\n",
    "    for page_num, page in enumerate(doc, start = 1):\n",
    "        page_blocks = []\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if block[\"type\"] == 0: # type 0 is text\n",
    "                text_content = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text_content:\n",
    "                    y = block[\"bbox\"][1]\n",
    "                    page_blocks.append({\n",
    "                        \"type\": \"TEXT DATA\",\n",
    "                        \"y\": y,\n",
    "                        \"content\": text_content\n",
    "                    })\n",
    "\n",
    "        # table extraction\n",
    "        try:\n",
    "            tables = camelot.read_pdf(str(pdf_file), pages = str(page_num), flavor = 'lattice') # lattice flavor to extract tables\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read tables on page {page_num}: {e}\")\n",
    "            tables = []\n",
    "\n",
    "        for table in tables:\n",
    "            table_data = table.data\n",
    "            bbox = table._bbox\n",
    "            y = float(bbox[1])\n",
    "            page_blocks.append({\n",
    "                \"type\": \"TABLE DATA\",\n",
    "                \"y\": y,\n",
    "                \"content\": table_data\n",
    "            })\n",
    "\n",
    "        page_blocks.sort(key=lambda b: b[\"y\"]) # sort contents on current page\n",
    "        result.extend(page_blocks) # append content to result list\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005acbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'TEXT DATA',\n",
       "  'y': 145.75482177734375,\n",
       "  'content': 'Follow the instructions contained herein, in addition to the general precautions to be observed while working. Even if the operator is already familiar with the use of manually operated lathes, it is necessary to: In particular:'},\n",
       " {'type': 'TEXT DATA', 'y': 173.48190307617188, 'content': 'fervi.com'},\n",
       " {'type': 'TEXT DATA',\n",
       "  'y': 188.8348388671875,\n",
       "  'content': '\\uf0b7 Acquire full knowledge of the machine. For safe operation, this manual must be read carefully in order to acquire the necessary knowledge of the machine and to understand: operation, safety devices and all necessary precautions. \\uf0b7 Wear appropriate clothing for the job. The operator must wear appropriate clothing to prevent accidents. \\uf0b7 Maintain the machine with care.'},\n",
       " {'type': 'TEXT DATA',\n",
       "  'y': 312.05987548828125,\n",
       "  'content': 'Risks associated with using the machine'},\n",
       " {'type': 'TEXT DATA',\n",
       "  'y': 342.43487548828125,\n",
       "  'content': 'The machine must only be used by personnel who have been specially trained by authorized personnel.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract texts and tables from the maual\n",
    "result = extract_text_and_tables(\"manual.pdf\")\n",
    "result[100:105] # few elements from the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27150c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'TABLE DATA',\n",
       " 'y': 91.1826731262468,\n",
       " 'content': [['Description (unit of measurement)', 'T999/230V\\nT999/400V'],\n",
       "  ['Centres distance (mm)', '1000'],\n",
       "  ['Spindle hole diameter (mm)', '38'],\n",
       "  ['Maximum swing over the bed (mm)', '320'],\n",
       "  ['Maximum swing over the cross slide (mm)', '198'],\n",
       "  ['Turning diameter over cavity (mm)', ''],\n",
       "  ['Spindle diameter (3 + 3 self centring) (mm)', ''],\n",
       "  ['Spindle connector', ''],\n",
       "  ['No. of spindle speeds', 'm'],\n",
       "  ['Spindle speed (r/min)', ''],\n",
       "  ['No. of metric threads', ''],\n",
       "  ['Range of metric threads (mm)', 'o'],\n",
       "  ['No. of inch threads', ''],\n",
       "  ['Range of inch threads (mm)', ''],\n",
       "  ['Range of longitudinal\\nfeeds (mm)', '00.78- 1.044\\nc'],\n",
       "  ['Range of transverse feeds (mm)', '0.022- 0.298'],\n",
       "  ['Outer diameter of the feed screw (mm)\\n.', '22'],\n",
       "  ['Guide length (mm)\\ni', '1390'],\n",
       "  ['Cross carriage travel (mm)\\nv', '200'],\n",
       "  ['Tailstock sleeve diameter (mm)', '32'],\n",
       "  ['Maximum travel of the tailstock sleeve (mm)\\nr', '80'],\n",
       "  ['Inner taper', 'CM 5'],\n",
       "  ['Tailstock base length (mm)\\ne', '165'],\n",
       "  ['Tailstock base width (mm)', '125'],\n",
       "  ['Steady rest diameter (mm)\\nf', '120'],\n",
       "  ['Dimensions (W x D x H) (mm)', '1820 x 530 x 1350'],\n",
       "  ['Package dimensions (W x D x H) (mm)', '1920 x 840 x 1560'],\n",
       "  ['Weight of machine (kg)', '520'],\n",
       "  ['Voltage / power supply frequency (V / Hz)', '230/50.\\n400 / 50'],\n",
       "  ['Motor power (W)', '1500/1800'],\n",
       "  [\"Acoustic pressure level at operator's workstation\\n(dB(A))\", '84']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample table data\n",
    "result[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f23a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list formatting by adding labels for texts and tables\n",
    "final = []\n",
    "for r in result:\n",
    "    s = f\"{r['type']}: {r['content']}\"\n",
    "    final.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f0b6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TABLE DATA: [[\\'Description (unit of measurement)\\', \\'T999/230V\\\\nT999/400V\\'], [\\'Centres distance (mm)\\', \\'1000\\'], [\\'Spindle hole diameter (mm)\\', \\'38\\'], [\\'Maximum swing over the bed (mm)\\', \\'320\\'], [\\'Maximum swing over the cross slide (mm)\\', \\'198\\'], [\\'Turning diameter over cavity (mm)\\', \\'\\'], [\\'Spindle diameter (3 + 3 self centring) (mm)\\', \\'\\'], [\\'Spindle connector\\', \\'\\'], [\\'No. of spindle speeds\\', \\'m\\'], [\\'Spindle speed (r/min)\\', \\'\\'], [\\'No. of metric threads\\', \\'\\'], [\\'Range of metric threads (mm)\\', \\'o\\'], [\\'No. of inch threads\\', \\'\\'], [\\'Range of inch threads (mm)\\', \\'\\'], [\\'Range of longitudinal\\\\nfeeds (mm)\\', \\'00.78- 1.044\\\\nc\\'], [\\'Range of transverse feeds (mm)\\', \\'0.022- 0.298\\'], [\\'Outer diameter of the feed screw (mm)\\\\n.\\', \\'22\\'], [\\'Guide length (mm)\\\\ni\\', \\'1390\\'], [\\'Cross carriage travel (mm)\\\\nv\\', \\'200\\'], [\\'Tailstock sleeve diameter (mm)\\', \\'32\\'], [\\'Maximum travel of the tailstock sleeve (mm)\\\\nr\\', \\'80\\'], [\\'Inner taper\\', \\'CM 5\\'], [\\'Tailstock base length (mm)\\\\ne\\', \\'165\\'], [\\'Tailstock base width (mm)\\', \\'125\\'], [\\'Steady rest diameter (mm)\\\\nf\\', \\'120\\'], [\\'Dimensions (W x D x H) (mm)\\', \\'1820 x 530 x 1350\\'], [\\'Package dimensions (W x D x H) (mm)\\', \\'1920 x 840 x 1560\\'], [\\'Weight of machine (kg)\\', \\'520\\'], [\\'Voltage / power supply frequency (V / Hz)\\', \\'230/50.\\\\n400 / 50\\'], [\\'Motor power (W)\\', \\'1500/1800\\'], [\"Acoustic pressure level at operator\\'s workstation\\\\n(dB(A))\", \\'84\\']]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table data sample after flattening\n",
    "final[156]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467abf8",
   "metadata": {},
   "source": [
    "It can be seen that the flattened version somewhat preserves the structure of the actual table by keeping each row inside a list. The LLM can hopefully understand this due to the presence of the label 'TABLE DATA' at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e902c0",
   "metadata": {},
   "source": [
    "### 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568a02b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 46\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TEXT DATA: 8.3 Levelling the machine TEXT DATA: For this operation, it is recommended to use a precision spirit level (0.001 mm). TEXT DATA: 8.3.1 Preliminary phase TEXT DATA: The preliminary phase serves to eliminate the presence of torsions in the lathe table. Proceed to reset the head by adjusting the relative screws and then locking the tailstock with the relative adjustment screws moving the reference mark to zero. TEXT DATA: fervi.com TEXT DATA: 8.3.2 Transverse levelling of the table TEXT DATA: Position the spirit level in a transverse direction on the lathe guides under the spindle and check the bubble. Position the spirit level in a transverse direction on the table guides under the tailstock and check the bubble. Repeat these operations frequently and, if necessary, make small corrections by screwing and/or unscrewing the adjustable feet below the pallet. TEXT DATA: 8.3.3 Levelling the lathe rails TEXT DATA: Place the level on the sides of the carriage and move it slowly along its entire length while checking that the bubble does not change. If the bubble moves, adjust the adjustable feet until it reaches a uniform level throughout the entire course of the carriage. Periodically check these measurements (at least every six months). TEXT DATA: Levelling the machine perfectly is one of the first and most essential steps to carry out before using the machine.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [\" \".join(final[i:i+10]) for i in range(0, len(final), 30)]\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "chunks[15] # sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3f6a3",
   "metadata": {},
   "source": [
    "### 3. Data cleaning for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115d6bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEXT DATA : 8.3 Levelling machine TEXT DATA : operation , recommended use precision spirit level ( 0.001 mm ) . TEXT DATA : 8.3.1 Preliminary phase TEXT DATA : preliminary phase serves eliminate presence torsions lathe table . Proceed reset head adjusting relative screws locking tailstock relative adjustment screws moving reference mark zero . TEXT DATA : fervi.com TEXT DATA : 8.3.2 Transverse levelling table TEXT DATA : Position spirit level transverse direction lathe guides spindle check bubble . Position spirit level transverse direction table guides tailstock check bubble . Repeat operations frequently , necessary , small corrections screwing and/or unscrewing adjustable feet pallet . TEXT DATA : 8.3.3 Levelling lathe rails TEXT DATA : Place level sides carriage slowly entire length checking bubble change . bubble moves , adjust adjustable feet reaches uniform level entire course carriage . Periodically check measurements ( months ) . TEXT DATA : Levelling machine perfectly essential steps carry machine .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "chunks_4_bm25 = []\n",
    "for chunk in chunks:\n",
    "    doc = nlp(chunk)\n",
    "    filtered = [token.text for token in doc if not token.is_stop]\n",
    "    chunks_4_bm25.append(\" \".join(filtered))\n",
    "\n",
    "chunks_4_bm25[15] # sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc45c9",
   "metadata": {},
   "source": [
    "### 4. Creating semantic vector embeddings and BM25 inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7480db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bert\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "sem_embs = model.encode(chunks, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d990dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bm25\n",
    "tokenized_corpus = [doc.split() for doc in chunks_4_bm25]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c06f4",
   "metadata": {},
   "source": [
    "### 5. Pipeline to return indices of top-K chunks that match with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0789fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_query_pipeline(query, top_k = 25):\n",
    "     \n",
    "    device = sem_embs.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs)[0] # cosine similarity\n",
    "    top_indices = np.argsort(cosine_scores.cpu().numpy())[::-1][:top_k]\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4542345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_query_pipeline(query, top_k = 25):\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query) # tf-idf like scoring\n",
    "    top_indices = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7364490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of staged chunks for context: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get common chunks from chunks retrived from both implementations\n",
    "\n",
    "# query = 'What are some general safety rules when using machine equipment?'\n",
    "query = 'What does the manual say about unplugging the power cord of the machine from the power outlet?'\n",
    "\n",
    "bert_top_k_idx = bert_query_pipeline(query)\n",
    "bm25_top_k_idx = bm25_query_pipeline(query)\n",
    "final_idx = list(set(list(bert_top_k_idx) + list(bm25_top_k_idx))) # union operation\n",
    "\n",
    "staged_context = [chunks[idx] for idx in final_idx]\n",
    "staged_context_4_bm25 = [chunks_4_bm25[idx] for idx in final_idx]\n",
    "print(f\"Number of staged chunks for context: {len(staged_context)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e600f5",
   "metadata": {},
   "source": [
    "### 6. Embed staged context using BERT & get inverted indices of staged context using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c2c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bert\n",
    "sem_embs_final = model.encode(staged_context, convert_to_tensor = True)\n",
    "\n",
    "# for bm25\n",
    "tokenized_corpus_final = [doc.split() for doc in staged_context_4_bm25]\n",
    "bm25_final = BM25Okapi(tokenized_corpus_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510463f",
   "metadata": {},
   "source": [
    "### 7. Function to get the final set of scores for the staged context chunks for both BERT & BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6edb56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_final_scores(query):\n",
    "     \n",
    "    device = sem_embs_final.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs_final)[0]\n",
    "    indices = np.argsort(cosine_scores.cpu().numpy())[::-1]\n",
    "\n",
    "    return cosine_scores.cpu().numpy(), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1843a4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.36677617, 0.27207935, 0.23525776, 0.38090566, 0.45139548,\n",
       "        0.27475065, 0.27165347, 0.18583837, 0.16829954, 0.15635544,\n",
       "        0.30504328, 0.45442048, 0.11290099, 0.4188665 , 0.3742811 ,\n",
       "        0.23033723, 0.43445706, 0.26141763, 0.23063627, 0.28430384,\n",
       "        0.24624479, 0.20206746, 0.34687048, 0.33071345, 0.1132399 ,\n",
       "        0.32948324, 0.31769413, 0.20785786, 0.2676775 , 0.29725122],\n",
       "       dtype=float32),\n",
       " array([11,  4, 16, 13,  3, 14,  0, 22, 23, 25, 26, 10, 29, 19,  5,  1,  6,\n",
       "        28, 17, 20,  2, 18, 15, 27, 21,  7,  8,  9, 24, 12], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_final_scores(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5ed7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_query_pipeline(query):\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25_final.get_scores(tokenized_query)\n",
    "    indices = np.argsort(bm25_scores)[::-1]\n",
    "\n",
    "    return bm25_scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e6ef3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4254599 , 0.        , 0.        , 1.30563606, 8.387102  ,\n",
       "        2.41920904, 4.00768191, 0.        , 0.        , 0.        ,\n",
       "        0.        , 5.40106626, 0.        , 1.51466801, 4.78826965,\n",
       "        1.19418756, 4.03768649, 0.78789749, 0.        , 7.23933463,\n",
       "        1.21193432, 0.        , 1.20878758, 1.00104951, 0.80055802,\n",
       "        0.92161524, 0.        , 0.        , 3.3932964 , 0.        ]),\n",
       " array([ 4, 19, 11, 14,  0, 16,  6, 28,  5, 13,  3, 20, 22, 15, 23, 25, 24,\n",
       "        17,  1,  2,  7, 27, 12,  8,  9, 10, 26, 18, 21, 29], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_query_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530ab50",
   "metadata": {},
   "source": [
    "### 8. Applying fusion scoring\n",
    "**`α * xi + (1 - α) * yi`**<br><br>\n",
    "...where `xi` is score of the ith chunk from the bert model and `yi` is score of the ith chunk from bm25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cad19d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize the scores\n",
    "def normalize_scores(scores):\n",
    "    min_s = np.min(scores)\n",
    "    max_s = np.max(scores)\n",
    "    return (scores - min_s) / (max_s - min_s) if max_s > min_s else scores\n",
    "\n",
    "# function to fuse the scores\n",
    "def fused_scores(query, alpha = 0.5, top_l = 10):\n",
    "    bm25_scores, bm25_indices = bm25_query_pipeline(query)\n",
    "    bert_scores, bert_indices = bert_final_scores(query)\n",
    "    \n",
    "    # create arrays to hold scores aligned by document index\n",
    "    num_docs = len(bm25_scores)  # should be same as bert_scores length\n",
    "    bm25_aligned = np.zeros(num_docs)\n",
    "    bert_aligned = np.zeros(num_docs)\n",
    "    \n",
    "    # align bm25 scores (indices are original document indices)\n",
    "    for idx, score in zip(bm25_indices, bm25_scores):\n",
    "        bm25_aligned[idx] = score\n",
    "\n",
    "    # align BERT scores\n",
    "    for idx, score in zip(bert_indices, bert_scores):\n",
    "        bert_aligned[idx] = score\n",
    "\n",
    "    # normalize\n",
    "    bm25_norm = normalize_scores(bm25_aligned)\n",
    "    bert_norm = normalize_scores(bert_aligned)\n",
    "\n",
    "    # fuse\n",
    "    fused = alpha * bm25_norm + (1 - alpha) * bert_norm\n",
    "\n",
    "    # top-L indices by fused score\n",
    "    top_indices = np.argsort(fused)[::-1][:top_l]\n",
    "\n",
    "    return top_indices\n",
    "\n",
    "best = fused_scores(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300c965",
   "metadata": {},
   "source": [
    "### 9. Get final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6366d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = ''\n",
    "for idx in best:\n",
    "    # remove unnecessary dots\n",
    "    final_string = re.sub(r'\\.{2,}', '.', chunks[idx])\n",
    "    final_context += final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4838151",
   "metadata": {},
   "source": [
    "### 10. LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c230273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama(prompt):\n",
    "    client = Groq(\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = \"llama-3.3-70b-versatile\",\n",
    "        # model = \"llama3-70b-8192\",\n",
    "        # model = \"mistral-saba-24b\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0.5,\n",
    "        max_tokens = 5640,\n",
    "        top_p = 1,\n",
    "        stream = True,\n",
    "    )\n",
    "\n",
    "    for chunk in chat_completion:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end='', flush=True)  # print to console without newline, flush immediately\n",
    "            time.sleep(0.01)  # optional delay for typewriter effect\n",
    "    \n",
    "\n",
    "def prompt(query, context):\n",
    "    return f\"\"\"\n",
    "        You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\n",
    "\n",
    "        Given the user question and the relevant extracted context from the manual:\n",
    "\n",
    "        - Provide a clear, precise, and factual answer to the question.\n",
    "        - Base your response strictly on the provided context; do not guess beyond it.\n",
    "        - If the context does not contain enough information, indicate that the answer is not available in the manual or that the context is not sufficient.\n",
    "        - Keep the answer professional, concise, and focused on practical instructions.\n",
    "        - Each section of the context begins with a tag: either 'TEXT DATA' or 'TABLE DATA'.\n",
    "        - 'TEXT DATA' represents plain, unstructured text. 'TABLE DATA' represents information extracted from a table and flattened into a list format.\n",
    "        - The 'TABLE DATA' is structured as a list of rows, where each row is a list containing the column values in order. The format is as follows: [[column 1 value, column 2 value, ...], [column 1 value, column 2 value, ...], ...]\n",
    "        \n",
    "        User Question:\n",
    "        {query}\n",
    "\n",
    "        Context from Manual:\n",
    "        {context}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779bc91",
   "metadata": {},
   "source": [
    "### 11. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c63ddb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the manual, the power cord of the machine should be unplugged from the power outlet in the following situations:\n",
      "\n",
      "1. When the machine is not being operated.\n",
      "2. When the machine is left unattended.\n",
      "3. During maintenance or registration, if the machine does not work properly.\n",
      "4. If the power cable is damaged.\n",
      "5. When replacing a tool.\n",
      "6. When the machine is being moved or transported.\n",
      "7. During cleaning operations.\n",
      "\n",
      "This information is provided in point 23 of the manual, which outlines the safety precautions to be taken when using the machine."
     ]
    }
   ],
   "source": [
    "prompt = prompt(query, final_context) # go to section number 5 to change query\n",
    "llama(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
