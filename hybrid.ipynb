{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216ea2fe",
   "metadata": {},
   "source": [
    "## **Hybrid Architecture: Fusion Retrieval + Reranking based RAG** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8aab1",
   "metadata": {},
   "source": [
    "_**What are your expectations for the Hybrid architecture with the provided manual?**_<br>\n",
    "The obvious answer would be that it will be better than both the approaches. Hybrid models in general tend to perform better than their individual constituent models. But this requires testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42cecd",
   "metadata": {},
   "source": [
    "**_How do you plan to test and compare these techniques?_**<br><br>\n",
    "<img src=\"./hybrid_workflow.png\" alt=\"Flowchart\" width=\"700\" /><br><br>\n",
    "The addition here as compared to the Fusion Retrieval architecture is the addition of an LLM based reranking approach right before asking an LLM to generate the response. In this case, there will be 3 sets of chunks throughout the workflow. First will be the set that is returned from the initial BERT embeddings and BM25 representations. This will be denoted as top-K. The next set of chunks will be based on the fusion scores from the semantic based retried chunks and keyword based retrieved chunks. This will be denoted as top-L. And finally, this set of L chunks will be passed into the first LLM for reranking. The chunks returned here will be the top-M chunks which will be the final set of context for our second and final LLM for query answering.<br>\n",
    "1. The document data is extracted, specifically text and tabular data. \n",
    "2. Next, these data are stored in a way where the sequence is maintained, that way there will be more context for a certain text that may have a table before or after it. \n",
    "3. This set is chunked and converted to BERT based vector embeddings and also to BM25 based representations.\n",
    "4. Now the query is also converted to BERT based embeddings and BM25 based representations and the top-K chunks are retrieved for both. A union is taken between the retrieved chunks to avoid overlaps.\n",
    "5. Next, this final set of chunks are again made into 2 copies, one of BERT based embeddings and the other of BM25 based inverted indexes. The query is passed into both structures and the scores are retrieved for each chunk. This gives us a set of chunk IDs, BERT scores, BM25 scores and the final fusion score. \n",
    "6. Based on this fusion score, the top-L chunks are retrieved (L < K). \n",
    "7. These top-L chunks are passed into the first LLM for reranking, which returns the top-M chunks based on its knowledge and understanding (M < L).\n",
    "8. This final set of top-M chunks are passed into the second LLM, along with the query for the final response generation.\n",
    "\n",
    "**It must be noted that K > L > M. In this implementation, they are set as 30, 15 and 5 respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4915c29",
   "metadata": {},
   "source": [
    "_**Comparison Strategy**_<br>\n",
    "There are possibly two main ways in which we can compare this approach with the Fusion Retrieval and Reranking approach. One is by assessing the top-M retrieved chunks from the Hybrid with the top-L retrieved chunks from both the other approaches. The other is obviously by assessing the final response from the LLM for each approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3b52f",
   "metadata": {},
   "source": [
    "_**Note**: Considering images is important in order to create a robust RAG system. However, due to technical/financial constraints, images are omitted for this implementation. However, in the absence of such constraints, what I would have done is have the LLM read the image and prompt it to generate a description. This description will be added into the resulting array while also maintaining the sequence. One obvious question in that case will be whether or not the LLM knows about the content in the image, provided that it is very domain specific and unfamiliar to the LLM. One way I thought of on mitigating this issue is by providing some set of surrounding context of the image to the LLM along with the image itself for it to draw better conclusions. These contexts can be the nearest 2 or 3 elements (text, table or another image) surrounding the image in hand. Let this value be J. So, if J is 3, we feed 3 elements before the image and 3 elements after the image as context for the LLM to generate proper a description of the image in hand. This might not be the most robust solution, but there can be scenarios where this will work._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb5dc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278daf5a",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # for text extraction\n",
    "import camelot # for table extraction\n",
    "from sentence_transformers import SentenceTransformer, util # for semantic vector embedding creation \n",
    "from rank_bm25 import BM25Okapi # for bm25 implementation\n",
    "import spacy # for stop word removal\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import os\n",
    "import time\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d0c0f",
   "metadata": {},
   "source": [
    "#### 1. Function to extract texts & tables from PDF\n",
    "The goal is to preserve the sequence, that way there will be more context for a certain text that may have a table before or after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_tables(pdf_path):\n",
    "\n",
    "    pdf_file = Path(pdf_path)\n",
    "    if not pdf_file.is_file() or pdf_file.suffix.lower() != \".pdf\":\n",
    "        raise FileNotFoundError(\"Provided file path is not a valid PDF.\")\n",
    "\n",
    "    doc = fitz.open(str(pdf_file))\n",
    "    result = []\n",
    "\n",
    "    # text extraction\n",
    "    for page_num, page in enumerate(doc, start = 1):\n",
    "        page_blocks = []\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if block[\"type\"] == 0: # type 0 is text\n",
    "                text_content = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text_content:\n",
    "                    y = block[\"bbox\"][1]\n",
    "                    page_blocks.append({\n",
    "                        \"type\": \"TEXT DATA\",\n",
    "                        \"y\": y,\n",
    "                        \"content\": text_content\n",
    "                    })\n",
    "\n",
    "        # table extraction\n",
    "        try:\n",
    "            tables = camelot.read_pdf(str(pdf_file), pages = str(page_num), flavor = 'lattice') # lattice flavor to extract tables\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read tables on page {page_num}: {e}\")\n",
    "            tables = []\n",
    "\n",
    "        for table in tables:\n",
    "            table_data = table.data\n",
    "            bbox = table._bbox\n",
    "            y = float(bbox[1])\n",
    "            page_blocks.append({\n",
    "                \"type\": \"TABLE DATA\",\n",
    "                \"y\": y,\n",
    "                \"content\": table_data\n",
    "            })\n",
    "\n",
    "        page_blocks.sort(key = lambda b: b[\"y\"]) # sort contents on current page\n",
    "        result.extend(page_blocks) # append content to result list\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract texts and tables from the maual\n",
    "pre_result = extract_text_and_tables(\"manual.pdf\")\n",
    "pre_result[100:105] # few elements from the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d52590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 'fervi.com' background text\n",
    "result = []\n",
    "for res in pre_result:\n",
    "    if res['content'] != 'fervi.com':\n",
    "        result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1672c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample table data\n",
    "result[1173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list formatting by adding labels for texts and tables\n",
    "final = []\n",
    "for r in result:\n",
    "    s = f\"{r['type']}: {r['content']}\"\n",
    "    final.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3172bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table data sample after flattening\n",
    "final[1173]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9288b",
   "metadata": {},
   "source": [
    "It can be seen that the flattened version somewhat preserves the structure of the actual table by keeping each row inside a list. The LLM can hopefully understand this due to the presence of the label 'TABLE DATA' at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7230f",
   "metadata": {},
   "source": [
    "### 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2888a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\" \".join(final[i:i + 20]) for i in range(0, len(final), 20)] # be careful here\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "chunks[15] # sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6bbd0",
   "metadata": {},
   "source": [
    "### 3. Data cleaning for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "chunks_4_bm25 = []\n",
    "for chunk in chunks:\n",
    "    doc = nlp(chunk)\n",
    "    filtered = [token.text for token in doc if not token.is_stop]\n",
    "    chunks_4_bm25.append(\" \".join(filtered))\n",
    "\n",
    "chunks_4_bm25[15] # sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ab81f",
   "metadata": {},
   "source": [
    "### 4. Creating semantic vector embeddings and BM25 inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c98f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bert\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "sem_embs = model.encode(chunks, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d255dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bm25\n",
    "tokenized_corpus = [doc.split() for doc in chunks_4_bm25]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f27932",
   "metadata": {},
   "source": [
    "### 5. Pipeline to return indices of top-K chunks that match with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_query_pipeline(query, top_k = 20):\n",
    "     \n",
    "    device = sem_embs.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs)[0] # cosine similarity\n",
    "    top_indices = np.argsort(cosine_scores.cpu().numpy())[::-1][:top_k]\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_query_pipeline(query, top_k = 20):\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query) # tf-idf like scoring\n",
    "    top_indices = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Summarize the manual.\" # 1\n",
    "# query = \"What are some general safety rules when using machine equipment?\" # 2\n",
    "# query = \"What does the manual say about unplugging the power cord of the machine from the power outlet?\"\" # 3\n",
    "# query = \"What are the several manual controls on the tool holder carriage?\" # 4\n",
    "query = \"Tell me about the lever for selection of longitudinal feeds.\" # 5\n",
    "# query = \"What does the document talk about regarding digital displays?\" # 6\n",
    "# query = \"What controls does the electric panel have?\" # 7\n",
    "# query = \"How to achieve balance when lifting the Lathe?\" # 8\n",
    "# query = \"Can I use the machine for turning non-ferrous materials?\" # 9\n",
    "# query = \"What should a grounding conductor be used for?\" # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common chunks from chunks retrived from both implementations\n",
    "\n",
    "bert_top_k_idx = bert_query_pipeline(query)\n",
    "bm25_top_k_idx = bm25_query_pipeline(query)\n",
    "final_idx = list(set(list(bert_top_k_idx) + list(bm25_top_k_idx))) # union operation\n",
    "\n",
    "staged_context = [chunks[idx] for idx in final_idx]\n",
    "staged_context_4_bm25 = [chunks_4_bm25[idx] for idx in final_idx]\n",
    "print(f\"Number of staged chunks for context: {len(staged_context)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d2610",
   "metadata": {},
   "source": [
    "### 6. Embed staged context using BERT & get inverted indices of staged context using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bert\n",
    "sem_embs_final = model.encode(staged_context, convert_to_tensor = True)\n",
    "\n",
    "# for bm25\n",
    "tokenized_corpus_final = [doc.split() for doc in staged_context_4_bm25]\n",
    "bm25_final = BM25Okapi(tokenized_corpus_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa723",
   "metadata": {},
   "source": [
    "### 7. Function to get the final set of scores for the staged context chunks for both BERT & BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aec529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_final_scores(query):\n",
    "     \n",
    "    device = sem_embs_final.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs_final)[0]\n",
    "    indices = np.argsort(cosine_scores.cpu().numpy())[::-1]\n",
    "\n",
    "    return cosine_scores.cpu().numpy(), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f729cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_final_scores(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_query_pipeline(query):\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25_final.get_scores(tokenized_query)\n",
    "    indices = np.argsort(bm25_scores)[::-1]\n",
    "\n",
    "    return bm25_scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_query_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70029e00",
   "metadata": {},
   "source": [
    "### 8. Applying fusion scoring\n",
    "**`α * xi + (1 - α) * yi`**<br><br>\n",
    "...where `xi` is score of the ith chunk from the bert model and `yi` is score of the ith chunk from bm25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize the scores\n",
    "def normalize_scores(scores):\n",
    "    min_s = np.min(scores)\n",
    "    max_s = np.max(scores)\n",
    "    return (scores - min_s) / (max_s - min_s) if max_s > min_s else scores\n",
    "\n",
    "# function to fuse the scores\n",
    "def fused_scores(query, alpha = 0.8, top_l = 15):\n",
    "    bm25_scores, bm25_indices = bm25_query_pipeline(query)\n",
    "    bert_scores, bert_indices = bert_final_scores(query)\n",
    "    \n",
    "    # create arrays to hold scores aligned by document index\n",
    "    num_docs = len(bm25_scores)  # should be same as bert_scores length\n",
    "    bm25_aligned = np.zeros(num_docs)\n",
    "    bert_aligned = np.zeros(num_docs)\n",
    "    \n",
    "    # align bm25 scores (indices are original document indices)\n",
    "    for idx, score in zip(bm25_indices, bm25_scores):\n",
    "        bm25_aligned[idx] = score\n",
    "\n",
    "    # align BERT scores\n",
    "    for idx, score in zip(bert_indices, bert_scores):\n",
    "        bert_aligned[idx] = score\n",
    "\n",
    "    # normalize\n",
    "    bm25_norm = normalize_scores(bm25_aligned)\n",
    "    bert_norm = normalize_scores(bert_aligned)\n",
    "\n",
    "    # fuse\n",
    "    fused = alpha * bm25_norm + (1 - alpha) * bert_norm\n",
    "\n",
    "    # top-L indices by fused score\n",
    "    top_indices = np.argsort(fused)[::-1][:top_l]\n",
    "\n",
    "    return top_indices\n",
    "\n",
    "best = fused_scores(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e44908",
   "metadata": {},
   "source": [
    "### 9. Get final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = ''\n",
    "for idx in best:\n",
    "    # remove unnecessary dots\n",
    "    final_string = re.sub(r'\\.{2,}', '.', chunks[idx])\n",
    "    final_context += f\"{final_string.strip()} (idx = {idx})\\n\\n\"\n",
    "\n",
    "final_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016328b",
   "metadata": {},
   "source": [
    "### 10. Setting up LLM A that returns indexes of top-M chunks based on the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_rank_contexts(query, final_context, top_m = 5):\n",
    "    context_string = \"\"\n",
    "    max_idx = len(final_context)\n",
    "    for i, ctx in enumerate(final_context):\n",
    "        context_string += f\"Context {i+1}:\\n{ctx.strip()}\\n\\n\"\n",
    "\n",
    "    return f\"\"\"\n",
    "        You are a highly skilled AI assistant that ranks technical contexts from a machinery operations and maintenance manual.\n",
    "\n",
    "        Your task is to rank the top-{top_m} most relevant contexts for answering the user’s question, based solely on the provided content.\n",
    "\n",
    "        Each context ends with a tag in the format: **(idx = N)**, where N is an integer between 0 and {max_idx - 1}. Do not guess or invent index values.\n",
    "\n",
    "        **Instructions**:\n",
    "        - Carefully read all the context snippets.\n",
    "        - Identify the most relevant contexts that directly support answering the question.\n",
    "        - Return exactly {top_m} `idx` values, in descending order of relevance (most relevant first).\n",
    "        - Only include integers in the range 0 to {max_idx - 1}.\n",
    "        - Format your answer as a comma-separated list of integers with no extra text.\n",
    "\n",
    "        **Example**: 12, 4, 7, 1, 0\n",
    "\n",
    "        User Question:\n",
    "        {query}\n",
    "\n",
    "        Candidate Contexts:\n",
    "        {context_string}\n",
    "    \"\"\"\n",
    "\n",
    "def llm_a(query, final_context): # mistral might not be the best but no option\n",
    "    prompt = prompt_rank_contexts(query, final_context)\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model = 'mistral', \n",
    "            prompt = prompt,\n",
    "            stream = False\n",
    "        )\n",
    "        output = response['response'].strip()\n",
    "\n",
    "        # parse the returned string for integer idx values\n",
    "        ranked_indexes = [int(idx.strip()) for idx in output.split(\",\") if idx.strip().isdigit()]\n",
    "        return ranked_indexes[:5]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error using Ollama (Python client):\", str(e))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of top-M chunks\n",
    "top_ids = llm_a(query, final_context)\n",
    "print(f\"IDs of top-M chunks are: {top_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4c39e",
   "metadata": {},
   "source": [
    "### 11. Get final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c57b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = ''\n",
    "for idx in top_ids:\n",
    "    final_context += chunks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f856f1f",
   "metadata": {},
   "source": [
    "### 12. Setting up LLM B for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b44c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_b(prompt):\n",
    "    client = Groq(\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = \"llama-3.3-70b-versatile\",\n",
    "        # model = \"llama3-70b-8192\",\n",
    "        # model = \"mistral-saba-24b\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0.5,\n",
    "        max_tokens = 5640,\n",
    "        top_p = 1,\n",
    "        stream = True,\n",
    "    )\n",
    "\n",
    "    for chunk in chat_completion:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end = '', flush = True)  # print to console without newline, flush immediately\n",
    "            time.sleep(0.01)  # optional tiny delay for typewriter effect\n",
    "    \n",
    "\n",
    "def prompt(query, context):\n",
    "    return f\"\"\"\n",
    "        You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\n",
    "\n",
    "        Given the user question and the relevant extracted context from the manual:\n",
    "\n",
    "        - Provide a clear, precise, and factual answer to the question.\n",
    "        - Base your response strictly on the provided context; do not guess beyond it.\n",
    "        - If the context does not contain enough information, indicate that the answer is not available in the manual or that the context is not sufficient.\n",
    "        - Keep the answer professional, concise, and focused on practical instructions.\n",
    "        - Each section of the context begins with a tag: either 'TEXT DATA' or 'TABLE DATA'.\n",
    "        - 'TEXT DATA' represents plain, unstructured text. 'TABLE DATA' represents information extracted from a table and flattened into a list format.\n",
    "        - The 'TABLE DATA' is structured as a list of rows, where each row is a list containing the column values in order. The format is as follows: [[column 1 value, column 2 value, ...], [column 1 value, column 2 value, ...], ...]\n",
    "        \n",
    "        User Question:\n",
    "        {query}\n",
    "\n",
    "        Context from Manual:\n",
    "        {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4b728",
   "metadata": {},
   "source": [
    "### 13. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt(query, final_context) # go to section number 5 to change query\n",
    "print(f\"QUERY: {query}\\n\")\n",
    "print('RESPONSE:')\n",
    "llm_b(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708cb10",
   "metadata": {},
   "source": [
    "### 12. Assessing the final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
